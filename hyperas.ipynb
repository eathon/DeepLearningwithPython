{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hyperas.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eathon/DeepLearningwithPython/blob/master/hyperas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "p26COp4rR8S3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 調參工具  Hyperas\n",
        " \n",
        "## Dataset - Fashion-MNIST\n",
        "\n",
        "![alt text](https://cdn-images-1.medium.com/max/1000/1*QQVbuP2SEasB0XAmvjW0AA.jpeg)"
      ]
    },
    {
      "metadata": {
        "id": "z_-jXEC_VB8K",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Code"
      ]
    },
    {
      "metadata": {
        "id": "jGb_LcJl0R9Q",
        "colab_type": "code",
        "outputId": "b38401bd-e855-4a77-eaee-f21639407c04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "cell_type": "code",
      "source": [
        "# sudo pip3 install --ignore-installed --upgrade tensorflow\n",
        "from __future__ import print_function\n",
        "import tensorflow as tf\n",
        "import keras.backend.tensorflow_backend as KTF\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "session = tf.Session(config=config)\n",
        "KTF.set_session(session)\n",
        "import keras\n",
        "#import tensorflow as tf\n",
        "print(keras.__version__)\n",
        "print(tf.__version__)\n",
        "# To ignore keep_dims warning\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2.2.4\n",
            "1.12.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0MGXn9L80l2C",
        "colab_type": "code",
        "outputId": "987b708a-7fec-49d7-c114-3ff53b4c6468",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1546
        }
      },
      "cell_type": "code",
      "source": [
        "# 安裝套件\n",
        "\n",
        "!pip install hyperas\n",
        "!pip install hyperopt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting hyperas\n",
            "  Downloading https://files.pythonhosted.org/packages/54/72/5533b6bf9b47dc33685c3e62c391d6eab5785a648a5ffa841e240a3db3fe/hyperas-0.4.tar.gz\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (from hyperas) (2.2.4)\n",
            "Collecting hyperopt (from hyperas)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ce/9f/f6324af3fc43f352e568b5850695c30ed7dd14af06a94f97953ff9187569/hyperopt-0.1.1-py3-none-any.whl (117kB)\n",
            "\u001b[K    100% |████████████████████████████████| 122kB 10.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: entrypoints in /usr/local/lib/python3.6/dist-packages (from hyperas) (0.2.3)\n",
            "Collecting jupyter (from hyperas)\n",
            "  Downloading https://files.pythonhosted.org/packages/83/df/0f5dd132200728a86190397e1ea87cd76244e42d39ec5e88efd25b2abd7e/jupyter-1.0.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.6/dist-packages (from hyperas) (4.4.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.6/dist-packages (from hyperas) (5.4.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras->hyperas) (1.0.6)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras->hyperas) (1.0.5)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras->hyperas) (1.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras->hyperas) (2.8.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras->hyperas) (1.14.6)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras->hyperas) (1.11.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras->hyperas) (3.13)\n",
            "Collecting pymongo (from hyperopt->hyperas)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b1/45/5440555b901a8416196fbf2499c4678ef74de8080c007104107a8cfdda20/pymongo-3.7.2-cp36-cp36m-manylinux1_x86_64.whl (408kB)\n",
            "\u001b[K    100% |████████████████████████████████| 409kB 15.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from hyperopt->hyperas) (0.16.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from hyperopt->hyperas) (2.2)\n",
            "Collecting qtconsole (from jupyter->hyperas)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e0/7a/8aefbc0ed078dec7951ac9a06dcd1869243ecd7bcbce26fa47bf5e469a8f/qtconsole-4.4.3-py2.py3-none-any.whl (113kB)\n",
            "\u001b[K    100% |████████████████████████████████| 122kB 37.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: notebook in /usr/local/lib/python3.6/dist-packages (from jupyter->hyperas) (5.2.2)\n",
            "Collecting jupyter-console (from jupyter->hyperas)\n",
            "  Downloading https://files.pythonhosted.org/packages/cb/ee/6374ae8c21b7d0847f9c3722dcdfac986b8e54fa9ad9ea66e1eb6320d2b8/jupyter_console-6.0.0-py2.py3-none-any.whl\n",
            "Collecting ipywidgets (from jupyter->hyperas)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/30/9a/a008c7b1183fac9e52066d80a379b3c64eab535bd9d86cdc29a0b766fd82/ipywidgets-7.4.2-py2.py3-none-any.whl (111kB)\n",
            "\u001b[K    100% |████████████████████████████████| 112kB 35.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: ipykernel in /usr/local/lib/python3.6/dist-packages (from jupyter->hyperas) (4.6.1)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.6/dist-packages (from nbformat->hyperas) (4.4.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from nbformat->hyperas) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.1 in /usr/local/lib/python3.6/dist-packages (from nbformat->hyperas) (4.3.2)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbformat->hyperas) (2.6.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (1.4.2)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (0.5.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (2.1.3)\n",
            "Requirement already satisfied: mistune>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (0.8.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (2.10)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (0.4.2)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (3.0.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->hyperopt->hyperas) (4.3.0)\n",
            "Requirement already satisfied: jupyter-client>=4.1 in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->hyperas) (5.2.3)\n",
            "Requirement already satisfied: tornado>=4 in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->hyperas) (4.5.3)\n",
            "Requirement already satisfied: terminado>=0.3.3; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->hyperas) (0.8.1)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.6/dist-packages (from jupyter-console->jupyter->hyperas) (5.5.0)\n",
            "Collecting prompt-toolkit<2.1.0,>=2.0.0 (from jupyter-console->jupyter->hyperas)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/e6/adb3be5576f5d27c6faa33f1e9fea8fe5dbd9351db12148de948507e352c/prompt_toolkit-2.0.7-py3-none-any.whl (338kB)\n",
            "\u001b[K    100% |████████████████████████████████| 348kB 34.4MB/s \n",
            "\u001b[?25hCollecting widgetsnbextension~=3.4.0 (from ipywidgets->jupyter->hyperas)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8a/81/35789a3952afb48238289171728072d26d6e76649ddc8b3588657a2d78c1/widgetsnbextension-3.4.2-py2.py3-none-any.whl (2.2MB)\n",
            "\u001b[K    100% |████████████████████████████████| 2.2MB 13.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2->nbconvert->hyperas) (1.1.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->hyperas) (0.5.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from jupyter-client>=4.1->qtconsole->jupyter->hyperas) (2.5.3)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.6/dist-packages (from jupyter-client>=4.1->qtconsole->jupyter->hyperas) (17.0.0)\n",
            "Requirement already satisfied: ptyprocess; os_name != \"nt\" in /usr/local/lib/python3.6/dist-packages (from terminado>=0.3.3; sys_platform != \"win32\"->notebook->jupyter->hyperas) (0.6.0)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython->jupyter-console->jupyter->hyperas) (0.8.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython->jupyter-console->jupyter->hyperas) (0.7.5)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython->jupyter-console->jupyter->hyperas) (4.6.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython->jupyter-console->jupyter->hyperas) (40.6.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->jupyter-console->jupyter->hyperas) (0.1.7)\n",
            "Building wheels for collected packages: hyperas\n",
            "  Running setup.py bdist_wheel for hyperas ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/06/38/3f/27826f57fae60ef788ceb47e2c649590ab8af31f42075325d2\n",
            "Successfully built hyperas\n",
            "\u001b[31mipython 5.5.0 has requirement prompt-toolkit<2.0.0,>=1.0.4, but you'll have prompt-toolkit 2.0.7 which is incompatible.\u001b[0m\n",
            "Installing collected packages: pymongo, hyperopt, qtconsole, prompt-toolkit, jupyter-console, widgetsnbextension, ipywidgets, jupyter, hyperas\n",
            "  Found existing installation: prompt-toolkit 1.0.15\n",
            "    Uninstalling prompt-toolkit-1.0.15:\n",
            "      Successfully uninstalled prompt-toolkit-1.0.15\n",
            "Successfully installed hyperas-0.4 hyperopt-0.1.1 ipywidgets-7.4.2 jupyter-1.0.0 jupyter-console-6.0.0 prompt-toolkit-2.0.7 pymongo-3.7.2 qtconsole-4.4.3 widgetsnbextension-3.4.2\n",
            "Requirement already satisfied: hyperopt in /usr/local/lib/python3.6/dist-packages (0.1.1)\n",
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.6/dist-packages (from hyperopt) (3.7.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from hyperopt) (1.11.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from hyperopt) (1.1.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from hyperopt) (2.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from hyperopt) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from hyperopt) (1.14.6)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->hyperopt) (4.3.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "krm_n0O06dj0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "! rm -rf hyperas.ipynb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CvAPxwCn0t0Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# See: https://stackoverflow.com/questions/49920031/get-the-path-of-the-notebook-on-google-colab\n",
        "# Install the PyDrive wrapper & import libraries.\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# Copy/download the file\n",
        "fid = drive.ListFile({'q':\"title='hyperas.ipynb'\"}).GetList()[0]['id']\n",
        "f = drive.CreateFile({'id': fid})\n",
        "f.GetContentFile('hyperas.ipynb')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gf1hIg5u05kf",
        "colab_type": "code",
        "outputId": "6afefd60-e531-4ed5-f53e-c650611db672",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        }
      },
      "cell_type": "code",
      "source": [
        "! ls -al"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 100\n",
            "drwxr-xr-x 1 root root  4096 Dec  8 14:37 .\n",
            "drwxr-xr-x 1 root root  4096 Dec  8 14:35 ..\n",
            "-rw-r--r-- 1 root root  2520 Dec  8 14:37 adc.json\n",
            "drwxr-xr-x 1 root root  4096 Dec  8 14:37 .config\n",
            "-rw-r--r-- 1 root root 72919 Dec  8 14:37 hyperas.ipynb\n",
            "drwxr-xr-x 2 root root  4096 Dec  5 17:39 sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4wHKwRKmpC6P",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Best model:\n",
        "{'Activation': 0, \n",
        "'Activation_1': 0, 'Dense': 2, 'Dense_1': 2, 'Dense_2': 3, 'Dropout': 0.020862912321193482, 'Dropout_1': 0.0910332772894481, 'Dropout_2': 0.8618283661109736, 'batch_size': 0, 'choiceval': 2, 'conditional': 0, 'lr': 0, 'lr_1': 1, 'lr_2': 0}\n"
      ]
    },
    {
      "metadata": {
        "id": "Faq1HLl81KsJ",
        "colab_type": "code",
        "outputId": "95dfd21e-8f79-4205-93f5-7ec66c8fb332",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 13168
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "from keras.datasets import mnist\n",
        "from keras.layers.core import Dense, Dropout, Activation\n",
        "from keras.models import Sequential\n",
        "\n",
        "\n",
        "from hyperas import optim\n",
        "from hyperas.distributions import choice, uniform, conditional\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.utils import np_utils\n",
        "from hyperopt import Trials, STATUS_OK, tpe\n",
        "  \n",
        "from keras import backend as K\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "\n",
        "X_test_global = None\n",
        "Y_test_global = None\n",
        "\n",
        "\n",
        "def data():\n",
        "  \n",
        " \n",
        "  from keras.datasets import fashion_mnist\n",
        "  from keras.datasets import mnist\n",
        "  from sklearn.model_selection import train_test_split\n",
        "\n",
        "  from keras.layers.core import Dense, Dropout, Activation\n",
        "  from keras.models import Sequential\n",
        "\n",
        "\n",
        "  from hyperas import optim\n",
        "  from hyperas.distributions import choice, uniform, conditional\n",
        "  from keras import backend as K\n",
        "\n",
        "  import seaborn as sns\n",
        "\n",
        "  sns.set()\n",
        "  \n",
        "  print('Data function')\n",
        "  img_rows, img_cols = 28, 28\n",
        "  nb_classes = 10\n",
        "  (X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "  X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2,random_state=12345)\n",
        "\n",
        "  pltsize=1\n",
        "  \n",
        "  print (X_train.shape)\n",
        "  \n",
        "  Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
        "  Y_val = np_utils.to_categorical(y_val, nb_classes)\n",
        "  Y_test= np_utils.to_categorical(y_test, nb_classes)\n",
        "  \n",
        "  \n",
        "  plt.figure(figsize=(10*pltsize, pltsize))\n",
        "\n",
        "  for i in range(10):\n",
        "\n",
        "    plt.subplot(1,10,i+1)\n",
        "    plt.axis('off')\n",
        "    plt.imshow(X_train[i,:,:], cmap=\"gray\")\n",
        "    plt.title('Class: '+str(y_train[i]))\n",
        "    print('Training sample',i,': class:',y_train[i], ', one-hot encoded:', Y_train[i])\n",
        "\n",
        "\n",
        "  X_train = X_train.reshape(X_train.shape[0], 784)\n",
        "  X_val = X_val.reshape(X_val.shape[0], 784)\n",
        "  X_test = X_test.reshape(X_test.shape[0], 784)\n",
        "  X_train = X_train.astype('float32')\n",
        "  X_val = X_val.astype('float32')\n",
        "  X_test = X_test.astype('float32')\n",
        "  X_train /= 255\n",
        "  X_val /= 255\n",
        "  X_test/= 255\n",
        "  \n",
        "  global X_test_global\n",
        "  X_test_global = X_test\n",
        "  global Y_test_global\n",
        "  Y_test_global= Y_test\n",
        "  \n",
        "  print (Y_test_global.shape)\n",
        "  \n",
        "  return X_train, Y_train, X_val, Y_val\n",
        "\n",
        "\n",
        "\n",
        "def create_model(X_train, Y_train, X_val, Y_val):\n",
        "    \"\"\"\n",
        "    Model providing function:\n",
        "\n",
        "    Create Keras model with double curly brackets dropped-in as needed.\n",
        "    Return value has to be a valid python dictionary with two customary keys:\n",
        "        - loss: Specify a numeric evaluation metric to be minimized\n",
        "        - status: Just use STATUS_OK and see hyperopt documentation if not feasible\n",
        "    The last one is optional, though recommended, namely:\n",
        "        - model: specify the model just created so that we can later use it again.\n",
        "    \n",
        "    {'Activation': 0, \n",
        "    'Activation_1': 0, \n",
        "    'Dense': 2, 'Dense_1': 2, 'Dense_2': 3, \n",
        "    'Dropout': 0.020862912321193482, 'Dropout_1': 0.0910332772894481, 'Dropout_2': 0.8618283661109736, \n",
        "    'batch_size': 0, \n",
        "    'choiceval': 2, 'conditional': 0, 'lr': 0, 'lr_1': 1, 'lr_2': 0}\n",
        "    \"\"\"\n",
        "    model = Sequential()\n",
        "    #layer 1\n",
        "    model.add(Dense({{choice([128, 256, 512, 1024])}}, input_shape=(784,)))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout({{uniform(0, 1)}}))\n",
        "    \n",
        "    # layer 2\n",
        "    model.add(Dense({{choice([128, 256, 512, 1024])}}))\n",
        "    model.add(Activation({{choice(['relu', 'sigmoid'])}}))\n",
        "    model.add(Dropout({{uniform(0, 1)}}))\n",
        "    \n",
        "    # layer 3\n",
        "    if conditional({{choice(['two', 'three'])}}) == 'three':\n",
        "        model.add(Dense({{choice([128, 256, 512, 1024])}}))\n",
        "        model.add(Activation({{choice(['relu', 'sigmoid'])}}))\n",
        "        model.add(Dropout({{uniform(0, 1)}}))\n",
        "    \n",
        "    model.add(Dense(10))\n",
        "    model.add(Activation('softmax'))\n",
        "    \n",
        "    \n",
        "    adam = keras.optimizers.Adam(lr={{choice([10**-3, 10**-4])}})\n",
        "    rmsprop = keras.optimizers.RMSprop(lr={{choice([10**-3, 10**-4])}})\n",
        "    sgd = keras.optimizers.SGD(lr={{choice([10**-3, 10**-4])}})\n",
        "   \n",
        "    choiceval = {{choice(['adam', 'sgd', 'rmsprop'])}}\n",
        "    if choiceval == 'adam':\n",
        "        optim = adam\n",
        "    elif choiceval == 'rmsprop':\n",
        "        optim = rmsprop\n",
        "    else:\n",
        "        optim = sgd\n",
        "        \n",
        "\n",
        "    model.compile(loss='categorical_crossentropy', metrics=['accuracy'],\n",
        "                  optimizer=optim)\n",
        "\n",
        "    model.fit(X_train, Y_train,\n",
        "              batch_size={{choice([32, 64, 128])}},\n",
        "              epochs=5,\n",
        "              verbose=2,\n",
        "              validation_data=(X_val, Y_val))\n",
        "    score, acc = model.evaluate(X_val, Y_val, verbose=0)\n",
        "    print('Test accuracy:', acc)\n",
        "    \n",
        "    return {'loss': -acc, 'status': STATUS_OK, 'model': model}\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    notebook_name='hyperas'\n",
        "    \n",
        "    X_train, Y_train, X_val, Y_val = data()\n",
        "    print ('Val set')\n",
        "    print (X_val.shape)\n",
        "    print (Y_val.shape)\n",
        "    best_run, best_model = optim.minimize(model=create_model,\n",
        "                                          data=data,\n",
        "                                          algo=tpe.suggest,\n",
        "                                          max_evals=30,\n",
        "                                          trials=Trials(),\n",
        "                                          notebook_name=notebook_name)\n",
        "    print (best_model.summary())\n",
        "    print(\"Evalutation of best performing model:\")\n",
        "    print(best_model.evaluate(  X_test_global, Y_test_global))\n",
        "    print(\"Best performing model chosen hyper-parameters:\")\n",
        "    print(best_run)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data function\n",
            "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 4us/step\n",
            "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 2s 0us/step\n",
            "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 1s 0us/step\n",
            "(48000, 28, 28)\n",
            "Training sample 0 : class: 3 , one-hot encoded: [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "Training sample 1 : class: 6 , one-hot encoded: [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "Training sample 2 : class: 4 , one-hot encoded: [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "Training sample 3 : class: 7 , one-hot encoded: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "Training sample 4 : class: 4 , one-hot encoded: [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "Training sample 5 : class: 9 , one-hot encoded: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "Training sample 6 : class: 0 , one-hot encoded: [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Training sample 7 : class: 9 , one-hot encoded: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "Training sample 8 : class: 0 , one-hot encoded: [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Training sample 9 : class: 3 , one-hot encoded: [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "(10000, 10)\n",
            "Val set\n",
            "(12000, 784)\n",
            "(12000, 10)\n",
            ">>> Imports:\n",
            "#coding=utf-8\n",
            "\n",
            "from __future__ import print_function\n",
            "\n",
            "try:\n",
            "    import tensorflow as tf\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import keras.backend.tensorflow_backend as KTF\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import keras\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from pydrive.auth import GoogleAuth\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from pydrive.drive import GoogleDrive\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from google.colab import auth\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from oauth2client.client import GoogleCredentials\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.datasets import mnist\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.layers.core import Dense, Dropout, Activation\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.models import Sequential\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from hyperas import optim\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from hyperas.distributions import choice, uniform, conditional\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import matplotlib.pyplot as plt\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.utils import np_utils\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from hyperopt import Trials, STATUS_OK, tpe\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras import backend as K\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import seaborn as sns\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.datasets import fashion_mnist\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.datasets import mnist\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from sklearn.model_selection import train_test_split\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.layers.core import Dense, Dropout, Activation\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.models import Sequential\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from hyperas import optim\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from hyperas.distributions import choice, uniform, conditional\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras import backend as K\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import seaborn as sns\n",
            "except:\n",
            "    pass\n",
            "\n",
            ">>> Hyperas search space:\n",
            "\n",
            "def get_space():\n",
            "    return {\n",
            "        'Dense': hp.choice('Dense', [128, 256, 512, 1024]),\n",
            "        'Dropout': hp.uniform('Dropout', 0, 1),\n",
            "        'Dense_1': hp.choice('Dense_1', [128, 256, 512, 1024]),\n",
            "        'Activation': hp.choice('Activation', ['relu', 'sigmoid']),\n",
            "        'Dropout_1': hp.uniform('Dropout_1', 0, 1),\n",
            "        'conditional': hp.choice('conditional', ['two', 'three']),\n",
            "        'Dense_2': hp.choice('Dense_2', [128, 256, 512, 1024]),\n",
            "        'Activation_1': hp.choice('Activation_1', ['relu', 'sigmoid']),\n",
            "        'Dropout_2': hp.uniform('Dropout_2', 0, 1),\n",
            "        'lr': hp.choice('lr', [10**-3, 10**-4]),\n",
            "        'lr_1': hp.choice('lr_1', [10**-3, 10**-4]),\n",
            "        'lr_2': hp.choice('lr_2', [10**-3, 10**-4]),\n",
            "        'choiceval': hp.choice('choiceval', ['adam', 'sgd', 'rmsprop']),\n",
            "        'batch_size': hp.choice('batch_size', [32, 64, 128]),\n",
            "    }\n",
            "\n",
            ">>> Data\n",
            "   1: \n",
            "   2: \n",
            "   3: \n",
            "   4: from keras.datasets import fashion_mnist\n",
            "   5: from keras.datasets import mnist\n",
            "   6: from sklearn.model_selection import train_test_split\n",
            "   7: \n",
            "   8: from keras.layers.core import Dense, Dropout, Activation\n",
            "   9: from keras.models import Sequential\n",
            "  10: \n",
            "  11: \n",
            "  12: from hyperas import optim\n",
            "  13: from hyperas.distributions import choice, uniform, conditional\n",
            "  14: from keras import backend as K\n",
            "  15: \n",
            "  16: import seaborn as sns\n",
            "  17: \n",
            "  18: sns.set()\n",
            "  19: \n",
            "  20: print('Data function')\n",
            "  21: img_rows, img_cols = 28, 28\n",
            "  22: nb_classes = 10\n",
            "  23: (X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
            "  24: \n",
            "  25: X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2,random_state=12345)\n",
            "  26: \n",
            "  27: pltsize=1\n",
            "  28: \n",
            "  29: print (X_train.shape)\n",
            "  30: \n",
            "  31: Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
            "  32: Y_val = np_utils.to_categorical(y_val, nb_classes)\n",
            "  33: Y_test= np_utils.to_categorical(y_test, nb_classes)\n",
            "  34: \n",
            "  35: \n",
            "  36: plt.figure(figsize=(10*pltsize, pltsize))\n",
            "  37: \n",
            "  38: for i in range(10):\n",
            "  39: \n",
            "  40:   plt.subplot(1,10,i+1)\n",
            "  41:   plt.axis('off')\n",
            "  42:   plt.imshow(X_train[i,:,:], cmap=\"gray\")\n",
            "  43:   plt.title('Class: '+str(y_train[i]))\n",
            "  44:   print('Training sample',i,': class:',y_train[i], ', one-hot encoded:', Y_train[i])\n",
            "  45: \n",
            "  46: \n",
            "  47: X_train = X_train.reshape(X_train.shape[0], 784)\n",
            "  48: X_val = X_val.reshape(X_val.shape[0], 784)\n",
            "  49: X_test = X_test.reshape(X_test.shape[0], 784)\n",
            "  50: X_train = X_train.astype('float32')\n",
            "  51: X_val = X_val.astype('float32')\n",
            "  52: X_test = X_test.astype('float32')\n",
            "  53: X_train /= 255\n",
            "  54: X_val /= 255\n",
            "  55: X_test/= 255\n",
            "  56: \n",
            "  57: global X_test_global\n",
            "  58: X_test_global = X_test\n",
            "  59: global Y_test_global\n",
            "  60: Y_test_global= Y_test\n",
            "  61: print (Y_test_global.shape)\n",
            "  62: \n",
            "  63: \n",
            "  64: \n",
            "  65: \n",
            ">>> Resulting replaced keras model:\n",
            "\n",
            "   1: def keras_fmin_fnct(space):\n",
            "   2: \n",
            "   3:     \"\"\"\n",
            "   4:     Model providing function:\n",
            "   5: \n",
            "   6:     Create Keras model with double curly brackets dropped-in as needed.\n",
            "   7:     Return value has to be a valid python dictionary with two customary keys:\n",
            "   8:         - loss: Specify a numeric evaluation metric to be minimized\n",
            "   9:         - status: Just use STATUS_OK and see hyperopt documentation if not feasible\n",
            "  10:     The last one is optional, though recommended, namely:\n",
            "  11:         - model: specify the model just created so that we can later use it again.\n",
            "  12:     \"\"\"\n",
            "  13:     model = Sequential()\n",
            "  14:     #layer 1\n",
            "  15:     model.add(Dense(space['Dense'], input_shape=(784,)))\n",
            "  16:     model.add(Activation('relu'))\n",
            "  17:     model.add(Dropout(space['Dropout']))\n",
            "  18:     \n",
            "  19:     # layer 2\n",
            "  20:     model.add(Dense(space['Dense_1']))\n",
            "  21:     model.add(Activation(space['Activation']))\n",
            "  22:     model.add(Dropout(space['Dropout_1']))\n",
            "  23:     \n",
            "  24:     # layer 3\n",
            "  25:     if conditional(space['conditional']) == 'three':\n",
            "  26:         model.add(Dense(space['Dense_2']))\n",
            "  27:         model.add(Activation(space['Activation_1']))\n",
            "  28:         model.add(Dropout(space['Dropout_2']))\n",
            "  29:     \n",
            "  30:     \n",
            "  31: \n",
            "  32:    \n",
            "  33: \n",
            "  34:     model.add(Dense(10))\n",
            "  35:     model.add(Activation('softmax'))\n",
            "  36:     \n",
            "  37:     \n",
            "  38:     adam = keras.optimizers.Adam(lr=space['lr'])\n",
            "  39:     rmsprop = keras.optimizers.RMSprop(lr=space['lr_1'])\n",
            "  40:     sgd = keras.optimizers.SGD(lr=space['lr_2'])\n",
            "  41:    \n",
            "  42:     choiceval = space['choiceval']\n",
            "  43:     if choiceval == 'adam':\n",
            "  44:         optim = adam\n",
            "  45:     elif choiceval == 'rmsprop':\n",
            "  46:         optim = rmsprop\n",
            "  47:     else:\n",
            "  48:         optim = sgd\n",
            "  49:         \n",
            "  50: \n",
            "  51:     model.compile(loss='categorical_crossentropy', metrics=['accuracy'],\n",
            "  52:                   optimizer=optim)\n",
            "  53: \n",
            "  54:     model.fit(X_train, Y_train,\n",
            "  55:               batch_size=space['batch_size'],\n",
            "  56:               epochs=5,\n",
            "  57:               verbose=2,\n",
            "  58:               validation_data=(X_val, Y_val))\n",
            "  59:     score, acc = model.evaluate(X_val, Y_val, verbose=0)\n",
            "  60:     print('Test accuracy:', acc)\n",
            "  61:     \n",
            "  62:     return {'loss': -acc, 'status': STATUS_OK, 'model': model}\n",
            "  63: \n",
            "Data function\n",
            "(48000, 28, 28)\n",
            "Training sample 0 : class: 3 , one-hot encoded: [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "Training sample 1 : class: 6 , one-hot encoded: [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "Training sample 2 : class: 4 , one-hot encoded: [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "Training sample 3 : class: 7 , one-hot encoded: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "Training sample 4 : class: 4 , one-hot encoded: [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "Training sample 5 : class: 9 , one-hot encoded: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "Training sample 6 : class: 0 , one-hot encoded: [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Training sample 7 : class: 9 , one-hot encoded: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "Training sample 8 : class: 0 , one-hot encoded: [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Training sample 9 : class: 3 , one-hot encoded: [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "(10000, 10)\n",
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/5\n",
            " - 4s - loss: 1.7711 - acc: 0.4845 - val_loss: 1.3744 - val_acc: 0.6496\n",
            "Epoch 2/5\n",
            " - 4s - loss: 1.2206 - acc: 0.6436 - val_loss: 1.0376 - val_acc: 0.6891\n",
            "Epoch 3/5\n",
            " - 4s - loss: 0.9939 - acc: 0.6870 - val_loss: 0.8837 - val_acc: 0.7242\n",
            "Epoch 4/5\n",
            " - 3s - loss: 0.8816 - acc: 0.7163 - val_loss: 0.7981 - val_acc: 0.7527\n",
            "Epoch 5/5\n",
            " - 4s - loss: 0.8140 - acc: 0.7371 - val_loss: 0.7432 - val_acc: 0.7622\n",
            "Test accuracy: 0.7621666666666667\n",
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/5\n",
            " - 4s - loss: 1.5962 - acc: 0.3821 - val_loss: 1.0463 - val_acc: 0.5710\n",
            "Epoch 2/5\n",
            " - 4s - loss: 1.3642 - acc: 0.4668 - val_loss: 0.8634 - val_acc: 0.6455\n",
            "Epoch 3/5\n",
            " - 4s - loss: 1.3057 - acc: 0.4947 - val_loss: 1.0342 - val_acc: 0.5791\n",
            "Epoch 4/5\n",
            " - 4s - loss: 1.2715 - acc: 0.5121 - val_loss: 0.8711 - val_acc: 0.6652\n",
            "Epoch 5/5\n",
            " - 4s - loss: 1.2651 - acc: 0.5141 - val_loss: 0.8853 - val_acc: 0.6478\n",
            "Test accuracy: 0.6478333333333334\n",
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/5\n",
            " - 2s - loss: 3.0027 - acc: 0.1021 - val_loss: 2.4737 - val_acc: 0.0912\n",
            "Epoch 2/5\n",
            " - 2s - loss: 2.9222 - acc: 0.1048 - val_loss: 2.4065 - val_acc: 0.0912\n",
            "Epoch 3/5\n",
            " - 2s - loss: 2.8694 - acc: 0.1068 - val_loss: 2.3590 - val_acc: 0.0912\n",
            "Epoch 4/5\n",
            " - 2s - loss: 2.8272 - acc: 0.1062 - val_loss: 2.3235 - val_acc: 0.0912\n",
            "Epoch 5/5\n",
            " - 2s - loss: 2.7989 - acc: 0.1081 - val_loss: 2.2950 - val_acc: 0.0967\n",
            "Test accuracy: 0.09666666666666666\n",
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/5\n",
            " - 2s - loss: 3.5153 - acc: 0.1011 - val_loss: 2.4093 - val_acc: 0.0978\n",
            "Epoch 2/5\n",
            " - 2s - loss: 3.4389 - acc: 0.1009 - val_loss: 2.3546 - val_acc: 0.0978\n",
            "Epoch 3/5\n",
            " - 2s - loss: 3.4249 - acc: 0.0999 - val_loss: 2.3322 - val_acc: 0.0978\n",
            "Epoch 4/5\n",
            " - 2s - loss: 3.3958 - acc: 0.1010 - val_loss: 2.3216 - val_acc: 0.0978\n",
            "Epoch 5/5\n",
            " - 2s - loss: 3.3660 - acc: 0.1025 - val_loss: 2.3168 - val_acc: 0.0978\n",
            "Test accuracy: 0.09783333333333333\n",
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/5\n",
            " - 4s - loss: 2.1015 - acc: 0.2470 - val_loss: 1.4593 - val_acc: 0.4910\n",
            "Epoch 2/5\n",
            " - 4s - loss: 1.6605 - acc: 0.3549 - val_loss: 1.2724 - val_acc: 0.5496\n",
            "Epoch 3/5\n",
            " - 4s - loss: 1.5396 - acc: 0.4019 - val_loss: 1.1448 - val_acc: 0.6159\n",
            "Epoch 4/5\n",
            " - 4s - loss: 1.5024 - acc: 0.4246 - val_loss: 1.0731 - val_acc: 0.6162\n",
            "Epoch 5/5\n",
            " - 4s - loss: 1.4661 - acc: 0.4422 - val_loss: 1.0566 - val_acc: 0.6048\n",
            "Test accuracy: 0.6048333333333333\n",
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/5\n",
            " - 8s - loss: 0.7821 - acc: 0.7364 - val_loss: 0.4755 - val_acc: 0.8388\n",
            "Epoch 2/5\n",
            " - 8s - loss: 0.5046 - acc: 0.8230 - val_loss: 0.4022 - val_acc: 0.8622\n",
            "Epoch 3/5\n",
            " - 8s - loss: 0.4440 - acc: 0.8427 - val_loss: 0.3719 - val_acc: 0.8718\n",
            "Epoch 4/5\n",
            " - 8s - loss: 0.4105 - acc: 0.8541 - val_loss: 0.3492 - val_acc: 0.8789\n",
            "Epoch 5/5\n",
            " - 8s - loss: 0.3854 - acc: 0.8647 - val_loss: 0.3422 - val_acc: 0.8818\n",
            "Test accuracy: 0.88175\n",
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/5\n",
            " - 4s - loss: 2.2818 - acc: 0.1222 - val_loss: 1.7317 - val_acc: 0.2074\n",
            "Epoch 2/5\n",
            " - 3s - loss: 1.9989 - acc: 0.1783 - val_loss: 1.7690 - val_acc: 0.2045\n",
            "Epoch 3/5\n",
            " - 3s - loss: 1.9271 - acc: 0.1911 - val_loss: 1.8546 - val_acc: 0.2235\n",
            "Epoch 4/5\n",
            " - 3s - loss: 1.8892 - acc: 0.2028 - val_loss: 1.8961 - val_acc: 0.2080\n",
            "Epoch 5/5\n",
            " - 3s - loss: 1.8642 - acc: 0.2119 - val_loss: 1.9781 - val_acc: 0.2073\n",
            "Test accuracy: 0.20725\n",
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/5\n",
            " - 3s - loss: 1.8415 - acc: 0.3175 - val_loss: 1.0807 - val_acc: 0.6139\n",
            "Epoch 2/5\n",
            " - 2s - loss: 1.0891 - acc: 0.5792 - val_loss: 0.7838 - val_acc: 0.7018\n",
            "Epoch 3/5\n",
            " - 2s - loss: 0.8623 - acc: 0.6779 - val_loss: 0.6642 - val_acc: 0.7502\n",
            "Epoch 4/5\n",
            " - 2s - loss: 0.7525 - acc: 0.7214 - val_loss: 0.6004 - val_acc: 0.7686\n",
            "Epoch 5/5\n",
            " - 2s - loss: 0.6912 - acc: 0.7446 - val_loss: 0.5598 - val_acc: 0.7854\n",
            "Test accuracy: 0.7854166666666667\n",
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/5\n",
            " - 3s - loss: 1.8240 - acc: 0.3293 - val_loss: 1.0319 - val_acc: 0.6578\n",
            "Epoch 2/5\n",
            " - 2s - loss: 1.2054 - acc: 0.5368 - val_loss: 0.7882 - val_acc: 0.6909\n",
            "Epoch 3/5\n",
            " - 2s - loss: 1.0192 - acc: 0.6063 - val_loss: 0.7014 - val_acc: 0.7238\n",
            "Epoch 4/5\n",
            " - 2s - loss: 0.9187 - acc: 0.6500 - val_loss: 0.6400 - val_acc: 0.7871\n",
            "Epoch 5/5\n",
            " - 2s - loss: 0.8611 - acc: 0.6711 - val_loss: 0.5965 - val_acc: 0.8012\n",
            "Test accuracy: 0.8011666666666667\n",
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/5\n",
            " - 9s - loss: 2.6022 - acc: 0.0979 - val_loss: 2.3111 - val_acc: 0.0652\n",
            "Epoch 2/5\n",
            " - 8s - loss: 2.4695 - acc: 0.1046 - val_loss: 2.3033 - val_acc: 0.1238\n",
            "Epoch 3/5\n",
            " - 8s - loss: 2.4184 - acc: 0.1077 - val_loss: 2.2952 - val_acc: 0.1482\n",
            "Epoch 4/5\n",
            " - 8s - loss: 2.3748 - acc: 0.1099 - val_loss: 2.2924 - val_acc: 0.1233\n",
            "Epoch 5/5\n",
            " - 8s - loss: 2.3530 - acc: 0.1121 - val_loss: 2.2930 - val_acc: 0.1155\n",
            "Test accuracy: 0.1155\n",
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/5\n",
            " - 3s - loss: 1.8695 - acc: 0.3381 - val_loss: 1.2503 - val_acc: 0.6031\n",
            "Epoch 2/5\n",
            " - 2s - loss: 1.1663 - acc: 0.5772 - val_loss: 0.8765 - val_acc: 0.6964\n",
            "Epoch 3/5\n",
            " - 2s - loss: 0.9185 - acc: 0.6559 - val_loss: 0.7338 - val_acc: 0.7187\n",
            "Epoch 4/5\n",
            " - 2s - loss: 0.7982 - acc: 0.7013 - val_loss: 0.6483 - val_acc: 0.7583\n",
            "Epoch 5/5\n",
            " - 2s - loss: 0.7228 - acc: 0.7308 - val_loss: 0.5895 - val_acc: 0.7788\n",
            "Test accuracy: 0.7788333333333334\n",
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/5\n",
            " - 3s - loss: 2.3820 - acc: 0.1238 - val_loss: 2.2719 - val_acc: 0.2191\n",
            "Epoch 2/5\n",
            " - 2s - loss: 2.2895 - acc: 0.1535 - val_loss: 2.1944 - val_acc: 0.2821\n",
            "Epoch 3/5\n",
            " - 2s - loss: 2.2198 - acc: 0.1850 - val_loss: 2.1308 - val_acc: 0.3670\n",
            "Epoch 4/5\n",
            " - 2s - loss: 2.1630 - acc: 0.2162 - val_loss: 2.0747 - val_acc: 0.4298\n",
            "Epoch 5/5\n",
            " - 2s - loss: 2.1099 - acc: 0.2481 - val_loss: 2.0231 - val_acc: 0.4643\n",
            "Test accuracy: 0.4643333333333333\n",
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/5\n",
            " - 4s - loss: 1.4622 - acc: 0.4696 - val_loss: 0.7824 - val_acc: 0.7072\n",
            "Epoch 2/5\n",
            " - 3s - loss: 0.7908 - acc: 0.7033 - val_loss: 0.5997 - val_acc: 0.7714\n",
            "Epoch 3/5\n",
            " - 3s - loss: 0.6516 - acc: 0.7576 - val_loss: 0.5199 - val_acc: 0.8098\n",
            "Epoch 4/5\n",
            " - 3s - loss: 0.5890 - acc: 0.7854 - val_loss: 0.4809 - val_acc: 0.8272\n",
            "Epoch 5/5\n",
            " - 3s - loss: 0.5433 - acc: 0.8038 - val_loss: 0.4479 - val_acc: 0.8401\n",
            "Test accuracy: 0.8400833333333333\n",
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/5\n",
            " - 4s - loss: 0.8213 - acc: 0.7109 - val_loss: 0.5169 - val_acc: 0.8207\n",
            "Epoch 2/5\n",
            " - 3s - loss: 0.5534 - acc: 0.8010 - val_loss: 0.4436 - val_acc: 0.8428\n",
            "Epoch 3/5\n",
            " - 3s - loss: 0.4923 - acc: 0.8227 - val_loss: 0.4126 - val_acc: 0.8538\n",
            "Epoch 4/5\n",
            " - 3s - loss: 0.4601 - acc: 0.8341 - val_loss: 0.3843 - val_acc: 0.8638\n",
            "Epoch 5/5\n",
            " - 3s - loss: 0.4395 - acc: 0.8402 - val_loss: 0.3648 - val_acc: 0.8730\n",
            "Test accuracy: 0.873\n",
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/5\n",
            " - 5s - loss: 2.7298 - acc: 0.0965 - val_loss: 2.4532 - val_acc: 0.0912\n",
            "Epoch 2/5\n",
            " - 4s - loss: 2.6100 - acc: 0.0988 - val_loss: 2.3647 - val_acc: 0.1104\n",
            "Epoch 3/5\n",
            " - 4s - loss: 2.5559 - acc: 0.1014 - val_loss: 2.3178 - val_acc: 0.1695\n",
            "Epoch 4/5\n",
            " - 4s - loss: 2.5213 - acc: 0.1044 - val_loss: 2.2914 - val_acc: 0.1899\n",
            "Epoch 5/5\n",
            " - 4s - loss: 2.5060 - acc: 0.1064 - val_loss: 2.2744 - val_acc: 0.2109\n",
            "Test accuracy: 0.21091666666666667\n",
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/5\n",
            " - 8s - loss: 0.7072 - acc: 0.7411 - val_loss: 0.4468 - val_acc: 0.8398\n",
            "Epoch 2/5\n",
            " - 7s - loss: 0.5392 - acc: 0.8017 - val_loss: 0.4025 - val_acc: 0.8488\n",
            "Epoch 3/5\n",
            " - 7s - loss: 0.5019 - acc: 0.8160 - val_loss: 0.3833 - val_acc: 0.8646\n",
            "Epoch 4/5\n",
            " - 7s - loss: 0.4860 - acc: 0.8246 - val_loss: 0.3726 - val_acc: 0.8651\n",
            "Epoch 5/5\n",
            " - 7s - loss: 0.4692 - acc: 0.8319 - val_loss: 0.3705 - val_acc: 0.8696\n",
            "Test accuracy: 0.8695833333333334\n",
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/5\n",
            " - 9s - loss: 2.3927 - acc: 0.1039 - val_loss: 2.2769 - val_acc: 0.1943\n",
            "Epoch 2/5\n",
            " - 8s - loss: 2.3458 - acc: 0.1077 - val_loss: 2.2551 - val_acc: 0.2290\n",
            "Epoch 3/5\n",
            " - 8s - loss: 2.3227 - acc: 0.1127 - val_loss: 2.2375 - val_acc: 0.2823\n",
            "Epoch 4/5\n",
            " - 8s - loss: 2.3009 - acc: 0.1195 - val_loss: 2.2210 - val_acc: 0.3355\n",
            "Epoch 5/5\n",
            " - 8s - loss: 2.2798 - acc: 0.1295 - val_loss: 2.2047 - val_acc: 0.3820\n",
            "Test accuracy: 0.382\n",
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/5\n",
            " - 3s - loss: 2.4202 - acc: 0.1229 - val_loss: 2.2525 - val_acc: 0.1941\n",
            "Epoch 2/5\n",
            " - 2s - loss: 2.3298 - acc: 0.1431 - val_loss: 2.1784 - val_acc: 0.2534\n",
            "Epoch 3/5\n",
            " - 2s - loss: 2.2630 - acc: 0.1646 - val_loss: 2.1202 - val_acc: 0.3197\n",
            "Epoch 4/5\n",
            " - 2s - loss: 2.2048 - acc: 0.1885 - val_loss: 2.0700 - val_acc: 0.3752\n",
            "Epoch 5/5\n",
            " - 2s - loss: 2.1607 - acc: 0.2100 - val_loss: 2.0244 - val_acc: 0.4202\n",
            "Test accuracy: 0.42025\n",
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/5\n",
            " - 3s - loss: 2.4670 - acc: 0.1199 - val_loss: 2.1830 - val_acc: 0.3570\n",
            "Epoch 2/5\n",
            " - 2s - loss: 2.2529 - acc: 0.1567 - val_loss: 2.1238 - val_acc: 0.4788\n",
            "Epoch 3/5\n",
            " - 2s - loss: 2.1985 - acc: 0.1807 - val_loss: 2.0594 - val_acc: 0.5447\n",
            "Epoch 4/5\n",
            " - 2s - loss: 2.1402 - acc: 0.2107 - val_loss: 1.9869 - val_acc: 0.5772\n",
            "Epoch 5/5\n",
            " - 2s - loss: 2.0926 - acc: 0.2349 - val_loss: 1.9119 - val_acc: 0.5881\n",
            "Test accuracy: 0.5880833333333333\n",
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/5\n",
            " - 4s - loss: 2.4152 - acc: 0.1014 - val_loss: 2.2670 - val_acc: 0.1452\n",
            "Epoch 2/5\n",
            " - 3s - loss: 2.3478 - acc: 0.1209 - val_loss: 2.1960 - val_acc: 0.1989\n",
            "Epoch 3/5\n",
            " - 3s - loss: 2.2871 - acc: 0.1437 - val_loss: 2.1337 - val_acc: 0.2279\n",
            "Epoch 4/5\n",
            " - 3s - loss: 2.2349 - acc: 0.1621 - val_loss: 2.0764 - val_acc: 0.2860\n",
            "Epoch 5/5\n",
            " - 3s - loss: 2.1799 - acc: 0.1870 - val_loss: 2.0223 - val_acc: 0.3509\n",
            "Test accuracy: 0.35091666666666665\n",
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/5\n",
            " - 10s - loss: 0.6028 - acc: 0.7937 - val_loss: 0.4287 - val_acc: 0.8571\n",
            "Epoch 2/5\n",
            " - 9s - loss: 0.4236 - acc: 0.8484 - val_loss: 0.3663 - val_acc: 0.8733\n",
            "Epoch 3/5\n",
            " - 9s - loss: 0.3741 - acc: 0.8646 - val_loss: 0.3364 - val_acc: 0.8850\n",
            "Epoch 4/5\n",
            " - 9s - loss: 0.3469 - acc: 0.8750 - val_loss: 0.3253 - val_acc: 0.8872\n",
            "Epoch 5/5\n",
            " - 9s - loss: 0.3287 - acc: 0.8818 - val_loss: 0.3137 - val_acc: 0.8912\n",
            "Test accuracy: 0.89125\n",
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/5\n",
            " - 10s - loss: 0.6164 - acc: 0.7893 - val_loss: 0.4311 - val_acc: 0.8493\n",
            "Epoch 2/5\n",
            " - 9s - loss: 0.4273 - acc: 0.8488 - val_loss: 0.3692 - val_acc: 0.8720\n",
            "Epoch 3/5\n",
            " - 9s - loss: 0.3805 - acc: 0.8642 - val_loss: 0.3506 - val_acc: 0.8742\n",
            "Epoch 4/5\n",
            " - 9s - loss: 0.3521 - acc: 0.8746 - val_loss: 0.3254 - val_acc: 0.8864\n",
            "Epoch 5/5\n",
            " - 9s - loss: 0.3334 - acc: 0.8804 - val_loss: 0.3173 - val_acc: 0.8898\n",
            "Test accuracy: 0.8898333333333334\n",
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/5\n",
            " - 11s - loss: 0.5155 - acc: 0.8114 - val_loss: 0.3976 - val_acc: 0.8543\n",
            "Epoch 2/5\n",
            " - 10s - loss: 0.3962 - acc: 0.8551 - val_loss: 0.3610 - val_acc: 0.8675\n",
            "Epoch 3/5\n",
            " - 10s - loss: 0.3599 - acc: 0.8682 - val_loss: 0.3908 - val_acc: 0.8550\n",
            "Epoch 4/5\n",
            " - 10s - loss: 0.3366 - acc: 0.8749 - val_loss: 0.3098 - val_acc: 0.8877\n",
            "Epoch 5/5\n",
            " - 10s - loss: 0.3205 - acc: 0.8819 - val_loss: 0.3479 - val_acc: 0.8708\n",
            "Test accuracy: 0.87075\n",
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/5\n",
            " - 10s - loss: 0.6292 - acc: 0.7818 - val_loss: 0.4325 - val_acc: 0.8548\n",
            "Epoch 2/5\n",
            " - 9s - loss: 0.4367 - acc: 0.8444 - val_loss: 0.3889 - val_acc: 0.8627\n",
            "Epoch 3/5\n",
            " - 9s - loss: 0.3914 - acc: 0.8608 - val_loss: 0.3523 - val_acc: 0.8772\n",
            "Epoch 4/5\n",
            " - 9s - loss: 0.3645 - acc: 0.8710 - val_loss: 0.3446 - val_acc: 0.8779\n",
            "Epoch 5/5\n",
            " - 9s - loss: 0.3475 - acc: 0.8760 - val_loss: 0.3279 - val_acc: 0.8851\n",
            "Test accuracy: 0.8850833333333333\n",
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/5\n",
            " - 10s - loss: 0.5708 - acc: 0.8073 - val_loss: 0.4167 - val_acc: 0.8582\n",
            "Epoch 2/5\n",
            " - 9s - loss: 0.4055 - acc: 0.8549 - val_loss: 0.3659 - val_acc: 0.8746\n",
            "Epoch 3/5\n",
            " - 9s - loss: 0.3623 - acc: 0.8696 - val_loss: 0.3378 - val_acc: 0.8812\n",
            "Epoch 4/5\n",
            " - 9s - loss: 0.3316 - acc: 0.8807 - val_loss: 0.3357 - val_acc: 0.8798\n",
            "Epoch 5/5\n",
            " - 9s - loss: 0.3131 - acc: 0.8874 - val_loss: 0.3081 - val_acc: 0.8915\n",
            "Test accuracy: 0.8915\n",
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/5\n",
            " - 10s - loss: 0.5780 - acc: 0.8024 - val_loss: 0.4299 - val_acc: 0.8524\n",
            "Epoch 2/5\n",
            " - 9s - loss: 0.4078 - acc: 0.8555 - val_loss: 0.3717 - val_acc: 0.8696\n",
            "Epoch 3/5\n",
            " - 9s - loss: 0.3627 - acc: 0.8696 - val_loss: 0.3502 - val_acc: 0.8794\n",
            "Epoch 4/5\n",
            " - 9s - loss: 0.3357 - acc: 0.8792 - val_loss: 0.3246 - val_acc: 0.8838\n",
            "Epoch 5/5\n",
            " - 9s - loss: 0.3156 - acc: 0.8866 - val_loss: 0.3077 - val_acc: 0.8938\n",
            "Test accuracy: 0.89375\n",
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/5\n",
            " - 12s - loss: 0.4970 - acc: 0.8206 - val_loss: 0.4157 - val_acc: 0.8438\n",
            "Epoch 2/5\n",
            " - 10s - loss: 0.3769 - acc: 0.8620 - val_loss: 0.3949 - val_acc: 0.8548\n",
            "Epoch 3/5\n",
            " - 10s - loss: 0.3409 - acc: 0.8737 - val_loss: 0.3621 - val_acc: 0.8675\n",
            "Epoch 4/5\n",
            " - 10s - loss: 0.3187 - acc: 0.8798 - val_loss: 0.3068 - val_acc: 0.8908\n",
            "Epoch 5/5\n",
            " - 10s - loss: 0.2993 - acc: 0.8872 - val_loss: 0.3563 - val_acc: 0.8691\n",
            "Test accuracy: 0.8690833333333333\n",
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/5\n",
            " - 11s - loss: 0.6012 - acc: 0.7946 - val_loss: 0.4484 - val_acc: 0.8397\n",
            "Epoch 2/5\n",
            " - 9s - loss: 0.4148 - acc: 0.8523 - val_loss: 0.3737 - val_acc: 0.8705\n",
            "Epoch 3/5\n",
            " - 9s - loss: 0.3698 - acc: 0.8679 - val_loss: 0.3340 - val_acc: 0.8828\n",
            "Epoch 4/5\n",
            " - 9s - loss: 0.3417 - acc: 0.8775 - val_loss: 0.3480 - val_acc: 0.8738\n",
            "Epoch 5/5\n",
            " - 9s - loss: 0.3222 - acc: 0.8833 - val_loss: 0.3178 - val_acc: 0.8887\n",
            "Test accuracy: 0.8886666666666667\n",
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/5\n",
            " - 11s - loss: 0.5943 - acc: 0.7966 - val_loss: 0.4412 - val_acc: 0.8498\n",
            "Epoch 2/5\n",
            " - 9s - loss: 0.4131 - acc: 0.8533 - val_loss: 0.3693 - val_acc: 0.8702\n",
            "Epoch 3/5\n",
            " - 9s - loss: 0.3684 - acc: 0.8665 - val_loss: 0.3392 - val_acc: 0.8807\n",
            "Epoch 4/5\n",
            " - 9s - loss: 0.3412 - acc: 0.8774 - val_loss: 0.3275 - val_acc: 0.8828\n",
            "Epoch 5/5\n",
            " - 9s - loss: 0.3223 - acc: 0.8831 - val_loss: 0.3102 - val_acc: 0.8929\n",
            "Test accuracy: 0.8929166666666667\n",
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/5\n",
            " - 11s - loss: 0.5985 - acc: 0.7970 - val_loss: 0.4171 - val_acc: 0.8598\n",
            "Epoch 2/5\n",
            " - 9s - loss: 0.4198 - acc: 0.8494 - val_loss: 0.3702 - val_acc: 0.8703\n",
            "Epoch 3/5\n",
            " - 9s - loss: 0.3732 - acc: 0.8659 - val_loss: 0.3411 - val_acc: 0.8792\n",
            "Epoch 4/5\n",
            " - 9s - loss: 0.3446 - acc: 0.8769 - val_loss: 0.3413 - val_acc: 0.8778\n",
            "Epoch 5/5\n",
            " - 9s - loss: 0.3245 - acc: 0.8832 - val_loss: 0.3154 - val_acc: 0.8881\n",
            "Test accuracy: 0.8880833333333333\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_87 (Dense)             (None, 512)               401920    \n",
            "_________________________________________________________________\n",
            "activation_87 (Activation)   (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout_62 (Dropout)         (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_88 (Dense)             (None, 512)               262656    \n",
            "_________________________________________________________________\n",
            "activation_88 (Activation)   (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout_63 (Dropout)         (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_89 (Dense)             (None, 10)                5130      \n",
            "_________________________________________________________________\n",
            "activation_89 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 669,706\n",
            "Trainable params: 669,706\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Evalutation of best performing model:\n",
            "10000/10000 [==============================] - 1s 69us/step\n",
            "[0.34851747982501985, 0.8761]\n",
            "Best performing model chosen hyper-parameters:\n",
            "{'Activation': 0, 'Activation_1': 0, 'Dense': 2, 'Dense_1': 2, 'Dense_2': 3, 'Dropout': 0.020862912321193482, 'Dropout_1': 0.0910332772894481, 'Dropout_2': 0.8618283661109736, 'batch_size': 0, 'choiceval': 2, 'conditional': 0, 'lr': 0, 'lr_1': 1, 'lr_2': 0}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlAAAABdCAYAAABq41iyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBSMEXelZB2YAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7fac52ebe7b8>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlAAAABdCAYAAABq41iyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBSMEXelZB2YAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7fabf6ffcf98>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "x5hBTHaG21Oa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!py3clean ."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UrXFIVZT278R",
        "colab_type": "code",
        "outputId": "e648d072-5ed2-4f9f-f55b-f1772395ba59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "! cat temp_model.py"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cat: temp_model.py: No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}